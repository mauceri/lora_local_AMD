{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbed2c1-02a2-467f-bf62-cd0c93b359c8",
   "metadata": {},
   "source": [
    "# LoRA sur AMD Radeon 680M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac84256-5594-4e6e-bc8d-3374455d3941",
   "metadata": {},
   "source": [
    "## Transformation corpus au format ChatML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ed7b61-a631-4721-9129-3ec749c895c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a81f87-3e11-4008-86d9-38de184773a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version.hip: 6.1.40091-a8dbc0c19\n",
      "env HSA_OVERRIDE_GFX_VERSION: 10.3.0\n",
      "CUDA available: True\n",
      "Device 0: AMD Radeon 680M\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "# --- Posez les variables AVANT d'importer torch ---\n",
    "os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"10.3.0\"\n",
    "os.environ[\"AMD_SERIALIZE_KERNEL\"] = \"3\"\n",
    "os.environ[\"PYTORCH_SDPA_ENABLE_HEURISTIC\"] = \"0\"\n",
    "os.environ[\"PYTORCH_SDPA_ALLOW_MATH\"] = \"1\"\n",
    "os.environ[\"PYTORCH_SDPA_ENABLE_FLASH\"] = \"0\"\n",
    "os.environ[\"PYTORCH_SDPA_ENABLE_MEM_EFFICIENT\"] = \"0\"\n",
    "os.environ[\"HIP_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"PYTORCH_HIP_ALLOC_CONF\"]=\"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "print(\"torch.version.hip:\", torch.version.hip)\n",
    "print(\"env HSA_OVERRIDE_GFX_VERSION:\", os.environ.get(\"HSA_OVERRIDE_GFX_VERSION\"))\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device 0:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0e0732-3bfa-4276-a169-eb90310b1218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK matmul: torch.Size([2, 128, 128]) cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3687/745649980.py:1: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)\n",
      "  x = torch.randn(2, 128, 128, device=\"cuda\")\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 128, 128, device=\"cuda\")\n",
    "y = x @ x.transpose(-1, -2)\n",
    "print(\"OK matmul:\", y.shape, y.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "930e91c8-f854-421a-b14b-5504f6e6af76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+rocm6.1\n",
      "/usr/local/lib/python3.10/dist-packages/torch/__init__.py\n",
      "4.57.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(torch.__file__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e16ac26-44d6-42e0-98f4-e9750838649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.0\n",
      "0.13.2\n",
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import transformers, peft, accelerate\n",
    "print(transformers.__version__)  # → 4.45.x (ou +)\n",
    "print(peft.__version__)          # → 0.12.x (ou +)\n",
    "print(accelerate.__version__)    # → 0.33.x (ou +)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b172714-3848-4233-aecf-6132d08c4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str) -> List[str]:\n",
    "    \"\"\"Charge les données ancien format.\"\"\"    \n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # ignore les lignes vides\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                data.append(obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Ligne {i} ignorée (erreur JSON) : {e}\")\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_data(path: str, data: List) -> None:\n",
    "    # Sauvegarde dans un fichier au format JSON Lines\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as fichier:\n",
    "        for objet in data:\n",
    "            fichier.write(json.dumps(objet, ensure_ascii=False) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328c14c8-0547-4200-97e9-c7869692597c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"théorème d'incomplétude\", \"vérité mathématique\", \"limites de la raison humaine\", \"récursivité\", \"auto-référence\"]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "old_train = load_data(\"data/old_train.jsonl\")\n",
    "\n",
    "chatml_train = []\n",
    "for e in old_train:\n",
    "    contenu = e.get(\"contenu\") or \"\"\n",
    "    user_text = (\n",
    "        \"Quelles sont les expressions clés contenues à l'identique dans ce texte : \"\n",
    "        + str(contenu)\n",
    "    )\n",
    "\n",
    "    # ⚠️ CIBLE D'APPRENTISSAGE : une CHAÎNE contenant un tableau JSON\n",
    "    # Exemple: '[\"accords de Bretton Woods\",\"dollar américain\"]'\n",
    "    target_list = e.get(\"expressions_clefs\") or []\n",
    "    assistant_text = json.dumps(list(map(str, target_list)), ensure_ascii=False)\n",
    "\n",
    "    ne = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"Vous êtes un extracteur d'expressions clés. Répondez UNIQUEMENT par un tableau JSON de chaînes, \"\n",
    "                    \"sans commentaire. Incluez UNIQUEMENT les expressions, dates et lieux remarquables, évènements, \"\n",
    "                    \"qui apparaissent à l'identique dans le texte.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_text},  # <- str, mais c'est un tableau JSON sérialisé\n",
    "        ]\n",
    "    }\n",
    "    chatml_train.append(ne)\n",
    "\n",
    "# Écrire le JSONL final\n",
    "save_data(\"data/train.jsonl\", chatml_train)\n",
    "\n",
    "# Vérif rapide : la cible est bien une chaîne qui ressemble à un tableau JSON\n",
    "print(chatml_train[0][\"messages\"][-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83055a8f-9f8c-4363-ab52-3f6ae5cc7b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Ligne 1027 ignorée (erreur JSON) : Invalid control character at: line 1 column 190 (char 189)\n",
      "[\"droits humains\", \"affaires internationales\", \"violations des droits\", \"normes des droits\", \"promotion des droits\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "old_test = load_data(\"data/old_test.jsonl\")\n",
    "\n",
    "chatml_test = []\n",
    "for e in old_test:\n",
    "    contenu = e.get(\"contenu\") or \"\"\n",
    "    user_text = (\n",
    "        \"Quelles sont les expressions clés contenues à l'identique dans ce texte : \"\n",
    "        + str(contenu)\n",
    "    )\n",
    "\n",
    "    # ⚠️ CIBLE D'APPRENTISSAGE : une CHAÎNE contenant un tableau JSON\n",
    "    # Exemple: '[\"accords de Bretton Woods\",\"dollar américain\"]'\n",
    "    target_list = e.get(\"expressions_clefs\") or []\n",
    "    assistant_text = json.dumps(list(map(str, target_list)), ensure_ascii=False)\n",
    "\n",
    "    ne = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"Vous êtes un extracteur d'expressions clés. Répondez UNIQUEMENT par un tableau JSON de chaînes, \"\n",
    "                    \"sans commentaire. Incluez UNIQUEMENT les expressions, dates et lieux remarquables, évènements, \"\n",
    "                    \"qui apparaissent à l'identique dans le texte.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_text},  # <- str, mais c'est un tableau JSON sérialisé\n",
    "        ]\n",
    "    }\n",
    "    chatml_test.append(ne)\n",
    "\n",
    "# Écrire le JSONL final\n",
    "save_data(\"data/test.jsonl\", chatml_test)\n",
    "\n",
    "# Vérif rapide : la cible est bien une chaîne qui ressemble à un tableau JSON\n",
    "print(chatml_test[0][\"messages\"][-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e59d2e-a372-46f6-94cf-3dca90f6e2ca",
   "metadata": {},
   "source": [
    "## Chargement dans un dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb52cac-4d89-4476-9bab-24e6f0a08918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5163ac429c40488fe162f492c63aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "ds = load_dataset(\"json\", data_files=\"data/train.jsonl\")  # sans features\n",
    "print(ds)  # doit montrer messages: list<struct<role: string, content: string>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59497738-1444-4cef-b9e0-45b4affc7a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': Value(dtype='string', id=None), 'content': Value(dtype='string', id=None)}]}\n",
      "{'messages': [{'role': 'system', 'content': \"Vous êtes un extracteur d'expressions clés. Répondez UNIQUEMENT par un tableau JSON de chaînes, sans commentaire. Incluez UNIQUEMENT les expressions, dates et lieux remarquables, évènements, qui apparaissent à l'identique dans le texte.\"}, {'role': 'user', 'content': \"Quelles sont les expressions clés contenues à l'identique dans ce texte : Les mathématiques de Gödel sont fascinantes et complexes. Son théorème d'incomplétude a profondément influencé la logique et les fondements des mathématiques. Il démontre qu'il existe des propositions mathématiques qui ne peuvent être ni prouvées ni réfutées au sein d'un système formel donné. Ce résultat remet en question la notion même de vérité mathématique et soulève des questions sur les limites de la raison humaine. En outre, Gödel explore des concepts tels que la récursivité et l'auto-référence, qui sont essentiels pour comprendre la structure des systèmes logiques. Sa pensée a également des implications en informatique, notamment dans la théorie des langages de programmation et les systèmes d'intelligence artificielle.\"}, {'role': 'assistant', 'content': '[\"théorème d\\'incomplétude\", \"vérité mathématique\", \"limites de la raison humaine\", \"récursivité\", \"auto-référence\"]'}]}\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"train\"].features)\n",
    "print(ds[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c98afa-ec5e-4273-9e04-577f8b6eb399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9981d7ef06ac4923a81e9db297225dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['messages', 'text'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16ac6006eae45109fddb10e1b94fc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 4890\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4890, dict_keys(['input_ids', 'attention_mask', 'labels']))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 0) Paramètres rapides ---\n",
    "ASSISTANT_TAG = \"<|assistant|>:\"   # balise devant la réponse\n",
    "MAX_LEN = 512                      # mettez 384 ou 256 si la VRAM est juste\n",
    "NUM_PROC = 4                       # parallel tokenization\n",
    "USE_PACKING = True                 # True = moins de padding -> plus rapide\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from typing import Dict, Any, List\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"models/phi4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, local_files_only=True)\n",
    "\n",
    "# Votre ds actuel (déjà chargé) :\n",
    "# ds = load_dataset(\"json\", data_files=\"data/train.jsonl\")\n",
    "assert isinstance(ds, DatasetDict) and \"train\" in ds\n",
    "\n",
    "# --- 1) messages -> text (chaîne plat) ---\n",
    "def messages_to_text(ex: Dict[str, Any]) -> Dict[str, str]:\n",
    "    msgs = ex.get(\"messages\", [])\n",
    "    # Formate: <|role|>: content\\n...\n",
    "    text = \"\\n\".join(f\"<|{m.get('role','user')}|>: {m.get('content','')}\" for m in msgs)\n",
    "    return {\"text\": text}\n",
    "\n",
    "ds_txt = ds.map(messages_to_text, num_proc=NUM_PROC)\n",
    "print(ds_txt)\n",
    "\n",
    "# --- 2) Tokenisation + labels assistant-only pré-calculés ---\n",
    "# (labels = -100 partout sauf après la DERNIÈRE occurrence de ASSISTANT_TAG)\n",
    "assert 'tokenizer' in globals(), \"Créez le tokenizer avant (AutoTokenizer.from_pretrained(...)).\"\n",
    "tpl_ids: List[int] = tokenizer.encode(ASSISTANT_TAG, add_special_tokens=False)\n",
    "\n",
    "def tok_and_mask(batch):\n",
    "    t = tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "    labels = []\n",
    "    for ids, attn in zip(t[\"input_ids\"], t[\"attention_mask\"]):\n",
    "        lab = [-100] * len(ids)\n",
    "        last = -1\n",
    "        for j in range(0, len(ids) - len(tpl_ids) + 1):\n",
    "            if ids[j:j+len(tpl_ids)] == tpl_ids:\n",
    "                last = j\n",
    "        if last >= 0:\n",
    "            start = last + len(tpl_ids)\n",
    "            end = max(i for i,a in enumerate(attn) if a == 1) + 1  # jusqu'au dernier token non-pad\n",
    "            lab[start:end] = ids[start:end]\n",
    "        labels.append(lab)\n",
    "    t[\"labels\"] = labels\n",
    "    return t\n",
    "\n",
    "remove_cols = [c for c in ds_txt[\"train\"].column_names if c != \"text\"]\n",
    "ds_tok = ds_txt.map(tok_and_mask, batched=True, num_proc=NUM_PROC, remove_columns=remove_cols)\n",
    "print(ds_tok)\n",
    "\n",
    "# --- 3) (Optionnel) Packing constant-length pour réduire le padding ---\n",
    "def pack_constant_length(ds_split, max_len: int):\n",
    "    big_ids, big_mask, big_lab = [], [], []\n",
    "    for rec in ds_split:\n",
    "        big_ids.extend(rec[\"input_ids\"])\n",
    "        big_mask.extend(rec[\"attention_mask\"])\n",
    "        big_lab.extend(rec[\"labels\"])\n",
    "    L = min(len(big_ids), len(big_mask), len(big_lab))\n",
    "    L = (L // max_len) * max_len\n",
    "    big_ids, big_mask, big_lab = big_ids[:L], big_mask[:L], big_lab[:L]\n",
    "    chunks = []\n",
    "    for i in range(0, L, max_len):\n",
    "        chunks.append({\n",
    "            \"input_ids\": big_ids[i:i+max_len],\n",
    "            \"attention_mask\": big_mask[i:i+max_len],\n",
    "            \"labels\": big_lab[i:i+max_len],\n",
    "        })\n",
    "    return Dataset.from_list(chunks)\n",
    "\n",
    "if USE_PACKING:\n",
    "    ds_packed = {}\n",
    "    for k in ds_tok.keys():\n",
    "        ds_packed[k] = pack_constant_length(ds_tok[k], MAX_LEN)\n",
    "    ds_packed = DatasetDict(ds_packed)\n",
    "    print(ds_packed)\n",
    "    train_dataset_final = ds_packed[\"train\"]\n",
    "    eval_dataset_final  = ds_packed.get(\"validation\")\n",
    "else:\n",
    "    train_dataset_final = ds_tok[\"train\"]\n",
    "    eval_dataset_final  = ds_tok.get(\"validation\")\n",
    "\n",
    "len(train_dataset_final), train_dataset_final[0].keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5248c26",
   "metadata": {},
   "source": [
    "\n",
    "# ⚡ Section: Pipeline rapide (pré-tokenisation + labels + packing)\n",
    "Cette section ajoute un flux **plus rapide** et **plus stable** pour l'entraînement **LoRA classique** :\n",
    "- Pré-calcul **une seule fois** des `labels` *assistant-only* (plus de scan par batch).\n",
    "- **Packing** de séquences en blocs de longueur fixe pour réduire le padding (optionnel).\n",
    "- Variables claires : `ds_raw` → `ds_txt` → `ds_tok` → `ds_packed`.\n",
    "- Exemple de `Trainer` qui réutilise `model` et `args` déjà définis dans votre notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1113247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Paramètres de cette section ---\n",
    "ASSISTANT_TAG = \"<|assistant|>:\"      # balise qui précède les réponses\n",
    "MAX_LEN = globals().get(\"MAX_LEN\", 512)  # fallback si non défini ailleurs\n",
    "NUM_PROC = 4                            # ajustez selon vos CPU\n",
    "USE_PACKING = True                      # True = réduit padding → accélère\n",
    "model_path = \"models/phi4\"\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# On suppose qu'un objet `ds` existe déjà avec au moins ds[\"train\"]\n",
    "# Sinon, adaptez ici en chargeant vos données dans ds_raw.\n",
    "try:\n",
    "    ds_raw = ds\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Aucun dataset `ds` trouvé dans le notebook. Définissez `ds` (DatasetDict) avant d'exécuter cette section.\")\n",
    "\n",
    "assert isinstance(ds_raw, (dict, DatasetDict)) and \"train\" in ds_raw, \"Le dataset `ds` doit être un DatasetDict avec une clé 'train'.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "190e2027-edc3-496f-9995-70e43207870a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53733673f21740a187797cef7f884757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,912,896 || all params: 3,844,934,656 || trainable%: 0.2318\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, DataCollatorWithPadding, Trainer\n",
    "\n",
    "# ----- 0) Libérer la VRAM si vous sortez d’un OOM -----\n",
    "def hard_free_gpu(*names):\n",
    "    import gc\n",
    "    G = globals()\n",
    "    for n in names or (\"trainer\",\"trainer_fast\",\"model\",\"peft_model\",\"lora\",\"optimizer\",\"scheduler\",\"dataloader\"):\n",
    "        if n in G:\n",
    "            obj = G[n]\n",
    "            try:\n",
    "                if hasattr(obj, \"model\") and hasattr(obj.model, \"to\"): obj.model.to(\"cpu\")\n",
    "            except: pass\n",
    "            try:\n",
    "                if hasattr(obj, \"to\"): obj.to(\"cpu\")\n",
    "            except: pass\n",
    "            try: del G[n]\n",
    "            except: pass\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        try: torch.cuda.empty_cache()\n",
    "        except: pass\n",
    "        if hasattr(torch.cuda, \"ipc_collect\"):\n",
    "            try: torch.cuda.ipc_collect()\n",
    "            except: pass\n",
    "\n",
    "hard_free_gpu()  # lancez-la juste après un OOM\n",
    "\n",
    "# ----- 1) Charger modèle & tokenizer (BF16) directement sur GPU -----\n",
    "model_path = \"models/phi4\"\n",
    "tok = AutoTokenizer.from_pretrained(model_path, use_fast=True, local_files_only=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "    tok.pad_token_id = tok.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map={\"\": 0},      # place directement chaque sous-module sur cuda:0\n",
    "    dtype=torch.bfloat16,    # (remplace torch_dtype=... déprécié)\n",
    "    low_cpu_mem_usage=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "# ROCm: évite SDPA “flash/mem-efficent” potentiellement lourd/instable\n",
    "try:\n",
    "    model.config.attn_implementation = \"eager\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "model.config.use_cache = False  # utile en train même sans checkpointing\n",
    "\n",
    "# ----- 2) Appliquer LoRA sur les bons modules -----\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "lora = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c76bfcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c740dc8d80f343c6b36e4d06c1836363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['messages', 'text'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Uniformiser en 'text': messages -> string, ou conserver si 'text' déjà présent.\n",
    "def messages_to_text(ex: Dict[str, Any]) -> str:\n",
    "    # si ex[\"messages\"] existe: concatène \"<|role|>: content\"\n",
    "    msgs = ex.get(\"messages\")\n",
    "    if isinstance(msgs, list) and msgs and isinstance(msgs[0], dict):\n",
    "        return \"\\n\".join(f\"<|{m.get('role','user')}|>: {m.get('content','')}\" for m in msgs)\n",
    "    # sinon, si 'text' existe déjà\n",
    "    if \"text\" in ex and isinstance(ex[\"text\"], str):\n",
    "        return ex[\"text\"]\n",
    "    # fallback: joindre toutes les valeurs str\n",
    "    return \" \".join(str(v) for v in ex.values() if isinstance(v, str))\n",
    "\n",
    "def add_text_column(ds_split):\n",
    "    return ds_split.map(lambda ex: {\"text\": messages_to_text(ex)}, num_proc=NUM_PROC)\n",
    "\n",
    "ds_txt = {}\n",
    "for k in ds_raw.keys():\n",
    "    ds_txt[k] = add_text_column(ds_raw[k])\n",
    "ds_txt = DatasetDict(ds_txt)\n",
    "print(ds_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e113cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4464d48ca679455386975caef400d38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Tokenisation + pré-calcul des 'labels' assistant-only\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "assert 'tokenizer' in globals(), \"Le tokenizer doit exister (variable `tokenizer`).\"\n",
    "\n",
    "tpl_ids: List[int] = tokenizer.encode(ASSISTANT_TAG, add_special_tokens=False)\n",
    "\n",
    "def tok_and_mask(batch):\n",
    "    t = tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "    labels = []\n",
    "    for ids, attn in zip(t[\"input_ids\"], t[\"attention_mask\"]):\n",
    "        lab = [-100] * len(ids)\n",
    "        last = -1\n",
    "        # chercher la dernière occurrence du template\n",
    "        for j in range(0, len(ids) - len(tpl_ids) + 1):\n",
    "            if ids[j:j+len(tpl_ids)] == tpl_ids:\n",
    "                last = j\n",
    "        if last >= 0:\n",
    "            start = last + len(tpl_ids)\n",
    "            # fin = dernier token non-pad\n",
    "            end = max(i for i, a in enumerate(attn) if a == 1) + 1\n",
    "            lab[start:end] = ids[start:end]\n",
    "        labels.append(lab)\n",
    "    t[\"labels\"] = labels\n",
    "    return t\n",
    "\n",
    "remove_cols = [c for c in ds_txt[\"train\"].column_names if c != \"text\"]\n",
    "ds_tok = {}\n",
    "for k in ds_txt.keys():\n",
    "    ds_tok[k] = ds_txt[k].map(tok_and_mask, batched=True, num_proc=NUM_PROC, remove_columns=remove_cols)\n",
    "ds_tok = DatasetDict(ds_tok)\n",
    "print(ds_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64e6465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 4890\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Packing constant-length (optionnel mais recommandé pour accélérer)\n",
    "def pack_constant_length(ds_split, max_len: int):\n",
    "    big_ids, big_mask, big_lab = [], [], []\n",
    "    for rec in ds_split:\n",
    "        big_ids.extend(rec[\"input_ids\"])\n",
    "        big_mask.extend(rec[\"attention_mask\"])\n",
    "        big_lab.extend(rec[\"labels\"])\n",
    "    # Taille minimale pour aligner labels/mask/ids\n",
    "    L = min(len(big_ids), len(big_mask), len(big_lab))\n",
    "    L = (L // max_len) * max_len\n",
    "    big_ids, big_mask, big_lab = big_ids[:L], big_mask[:L], big_lab[:L]\n",
    "    # Re-chunk\n",
    "    from datasets import Dataset\n",
    "    chunks = []\n",
    "    for i in range(0, L, max_len):\n",
    "        chunks.append({\n",
    "            \"input_ids\": big_ids[i:i+max_len],\n",
    "            \"attention_mask\": big_mask[i:i+max_len],\n",
    "            \"labels\": big_lab[i:i+max_len],\n",
    "        })\n",
    "    return Dataset.from_list(chunks)\n",
    "\n",
    "if USE_PACKING:\n",
    "    ds_packed = {}\n",
    "    for k in ds_tok.keys():\n",
    "        ds_packed[k] = pack_constant_length(ds_tok[k], MAX_LEN)\n",
    "    from datasets import DatasetDict\n",
    "    ds_packed = DatasetDict(ds_packed)\n",
    "    print(ds_packed)\n",
    "    train_dataset_final = ds_packed[\"train\"]\n",
    "    eval_dataset_final  = ds_packed.get(\"validation\")\n",
    "else:\n",
    "    train_dataset_final = ds_tok[\"train\"]\n",
    "    eval_dataset_final  = ds_tok.get(\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13358ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Trainer: on réutilise `model` et `args` s'ils existent déjà.\n",
    "from transformers import TrainingArguments, DataCollatorWithPadding, Trainer\n",
    "\n",
    "if \"args\" not in globals():\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./checkpoints\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-5,\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        fp16=False,\n",
    "        optim=\"adamw_torch\",\n",
    "        dataloader_num_workers=2,\n",
    "        dataloader_pin_memory=False,\n",
    "    )\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "trainer_fast = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset_final,\n",
    "    eval_dataset=eval_dataset_final,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "print(\"Batches/train:\", len(trainer_fast.get_train_dataloader()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651db67",
   "metadata": {},
   "source": [
    "\n",
    "### Lancer l'entraînement rapide\n",
    "Exécutez la cellule ci-dessous pour entraîner avec le pipeline optimisé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6527f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_result = trainer_fast.train()\n",
    "print(train_result)\n",
    "\n",
    "if trainer_fast.eval_dataset is not None:\n",
    "    metrics = trainer_fast.evaluate()\n",
    "    print(metrics)\n",
    "\n",
    "trainer_fast.save_model(\"./checkpoints-fast\")\n",
    "tokenizer.save_pretrained(\"./checkpoints-fast\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1e836cb-53c3-4b8b-9488-2114636a0709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Modèle : models/phi4\n",
      "[i] Répertoire d’adaptateur (base) : checkpoints_phi4_lora\n",
      "[i] Adaptateur résolu : checkpoints_phi4_lora\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60da965ce0e40a796abc1e508f527e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Adaptateur LoRA chargé.\n",
      "[i] Adaptateur fusionné (merge_and_unload) pour inférence.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6917449eaa2c43b293dab7b64ff78918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea10062d86d4a24bee7094eadcb1fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenisation test:   0%|          | 0/1026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1e58699bc448379d83bd19a768a9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Évaluation (loss/ppl):   0%|          | 0/513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Mean loss: 2.4405 | Perplexity: 11.48\n",
      "\n",
      "[Exemple de génération]\n",
      "Vous êtes un extracteur d'expressions clés. Répondez UNIQUEMENT par un tableau JSON de chaînes, sans commentaire. Incluez UNIQUEMENT les expressions, dates et lieux remarquables, évènements, qui apparaissent à l'identique dans le texte.\n",
      "Quelles sont les expressions clés contenues à l'identique dans ce texte : Les droits humains sont des principes fondamentaux qui garantissent la dignité et l'égalité de tous les individus, sans distinction. Dans le cadre des affaires internationales, le respect de ces droits est souvent au cœur des discussions diplomatiques. Les violations des droits humains peuvent conduire à des sanctions, des interventions ou des résolutions au sein d'organisations telles que l'ONU. Les traités internationaux, comme la Déclaration universelle des droits de l'homme, établissent des normes que les États doivent respecter. La promotion des droits humains est essentielle pour maintenir la paix et la sécurité mondiale. Les organisations non gouvernementales jouent un rôle crucial dans la sensibilisation et le plaidoyer en faveur de ces droits. Les droits humains sont non seulement des droits individuels mais aussi des droits collectifs qui nécessitent une protection internationale.\" [\"droits humains\", \"principes fondamentaux\", \"dignité et l'égalité\", \"violations des droits humains\", \"traités internationaux\", \"promouvoir les droits\"]```json\n",
      "\n",
      "[\"droits humains\", \"principes fondamentaux\", \"dignité et l'égalité\", \"violations des droits humains\", \"traités internationaux\", \"promouvoir les droits\"]\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# === Évaluation avec barres de progression ===\n",
    "import os, json, math, torch, warnings, glob\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm  # <- barre de progression Jupyter/terminal\n",
    "\n",
    "MODEL_PATH = model_path if \"model_path\" in globals() else \"models/phi4\"\n",
    "BASE_ADAPTER_DIR = \"checkpoints_phi4_lora\"  # <- votre dossier effectif\n",
    "TEST_FILE = \"data/test.jsonl\"\n",
    "MAX_LEN_EVAL = 1024\n",
    "BATCH = 2\n",
    "\n",
    "assert Path(TEST_FILE).exists(), f\"{TEST_FILE} introuvable.\"\n",
    "print(f\"[i] Modèle : {MODEL_PATH}\")\n",
    "print(f\"[i] Répertoire d’adaptateur (base) : {BASE_ADAPTER_DIR}\")\n",
    "\n",
    "def find_adapter_dir(root: str) -> str | None:\n",
    "    if Path(root, \"adapter_config.json\").exists():\n",
    "        return root\n",
    "    cands = glob.glob(os.path.join(root, \"**\", \"adapter_config.json\"), recursive=True)\n",
    "    return str(Path(cands[0]).parent) if cands else None\n",
    "\n",
    "ADAPTER_DIR = find_adapter_dir(BASE_ADAPTER_DIR)\n",
    "print(f\"[i] Adaptateur résolu : {ADAPTER_DIR or '(aucun, modèle de base)'}\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True, local_files_only=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "    tok.pad_token_id = tok.eos_token_id\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=(torch.bfloat16 if torch.cuda.is_available() else torch.float32),\n",
    "    low_cpu_mem_usage=True,\n",
    "    local_files_only=True,\n",
    ").to(device)\n",
    "\n",
    "from peft import PeftModel\n",
    "if ADAPTER_DIR:\n",
    "    try:\n",
    "        model = PeftModel.from_pretrained(model, ADAPTER_DIR, is_trainable=False)\n",
    "        print(\"[i] Adaptateur LoRA chargé.\")\n",
    "        try:\n",
    "            model = model.merge_and_unload()\n",
    "            print(\"[i] Adaptateur fusionné (merge_and_unload) pour inférence.\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Échec du chargement de l’adaptateur : {e}\")\n",
    "else:\n",
    "    warnings.warn(\"Aucun adaptateur détecté — utilisation du modèle de base.\")\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "try: model.config.attn_implementation = \"eager\"\n",
    "except Exception: pass\n",
    "\n",
    "# --------- Chargement + tokenisation avec barre de progression ---------\n",
    "def join_chatml(msgs):\n",
    "    return \"\\n\".join(f\"<|{m['role']}|> {m['content']}\" for m in msgs)\n",
    "\n",
    "ds_test = load_dataset(\"json\", data_files=TEST_FILE, split=\"train\")\n",
    "\n",
    "def tok_fn(ex):\n",
    "    text = join_chatml(ex[\"messages\"])\n",
    "    return tok(text, truncation=True, max_length=MAX_LEN_EVAL, padding=False)\n",
    "\n",
    "# La lib `datasets` affiche déjà une barre, on renforce avec desc explicite :\n",
    "tok_ds = ds_test.map(tok_fn, remove_columns=[\"messages\"], desc=\"Tokenisation test\")\n",
    "\n",
    "if \"labels\" in tok_ds.column_names:\n",
    "    tok_ds = tok_ds.remove_columns(\"labels\")\n",
    "\n",
    "def lm_collate(batch):\n",
    "    padded = tok.pad(batch, padding=True, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "    labels = padded[\"input_ids\"].clone()\n",
    "    if \"attention_mask\" in padded:\n",
    "        labels[padded[\"attention_mask\"] == 0] = -100\n",
    "    else:\n",
    "        labels[labels == tok.pad_token_id] = -100\n",
    "    padded[\"labels\"] = labels\n",
    "    return padded\n",
    "\n",
    "dl = DataLoader(tok_ds, batch_size=BATCH, shuffle=False, collate_fn=lm_collate)\n",
    "\n",
    "# --------- Boucle d’évaluation avec barre de progression ---------\n",
    "loss_sum, count = 0.0, 0\n",
    "for batch in tqdm(dl, desc=\"Évaluation (loss/ppl)\"):\n",
    "    with torch.no_grad():\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model(**batch)\n",
    "        loss = out.loss\n",
    "        if not torch.isnan(loss):\n",
    "            loss_sum += float(loss.item())\n",
    "            count += 1\n",
    "\n",
    "if count:\n",
    "    mean_loss = loss_sum / count\n",
    "    ppl = math.exp(mean_loss) if mean_loss < 20 else float(\"inf\")\n",
    "    print(f\"[✓] Mean loss: {mean_loss:.4f} | Perplexity: {ppl:.2f}\")\n",
    "else:\n",
    "    print(\"[!] Aucun batch valide pour la perte.\")\n",
    "\n",
    "# --------- Génération (pas de barre utile pour un seul exemple) ---------\n",
    "example_msgs = ds_test[0][\"messages\"]\n",
    "prompt = join_chatml(example_msgs[:-1])\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "    )\n",
    "\n",
    "print(\"\\n[Exemple de génération]\")\n",
    "print(tok.decode(gen[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7845f8a-295f-4e09-b8a4-533949aa7e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Chargement du modèle : models/phi4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0b260c72b049309fdeeb0378c20715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Chargement adaptateur LoRA depuis checkpoints_phi4_lora\n",
      "[i] Fusion (merge_and_unload)...\n",
      "[✓] Modèle fusionné sauvegardé : gguf_out/phi4_merged/merged_hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: merged_hf\n",
      "INFO:hf-to-gguf:Model architecture: Phi3ForCausalLM\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_factors_long.weight,  torch.float32 --> F32, shape = {48}\n",
      "INFO:hf-to-gguf:rope_factors_short.weight, torch.float32 --> F32, shape = {48}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.float32 --> F16, shape = {3072, 200064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 5120}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 199742 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 199999\n",
      "INFO:gguf.vocab:Setting special token type eos to 199999\n",
      "INFO:gguf.vocab:Setting special token type unk to 199999\n",
      "INFO:gguf.vocab:Setting special token type pad to 199999\n",
      "INFO:gguf.vocab:Setting add_bos_token to False\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'system' and 'tools' in message and message['tools'] is not none %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|tool|>' + message['tools'] + '<|/tool|>' + '<|end|>' }}{% else %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% else %}{{ eos_token }}{% endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:gguf_out/phi4_merged/model-f16.gguf: n_tensors = 196, total_size = 7.7G\n",
      "Writing: 100%|██████████| 7.67G/7.67G [00:25<00:00, 306Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to gguf_out/phi4_merged/model-f16.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Conversion GGUF f16 : gguf_out/phi4_merged/model-f16.gguf (7680.7 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main: build = 0 (unknown)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing 'gguf_out/phi4_merged/model-f16.gguf' to 'gguf_out/phi4_merged/model-Q4_K_M.gguf' as Q4_K_M\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 196 tensors from gguf_out/phi4_merged/model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
      "llama_model_loader: - kv   2:                               general.type str              = model\n",
      "llama_model_loader: - kv   3:                               general.name str              = Merged_Hf\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 3.8B\n",
      "llama_model_loader: - kv   5:                        phi3.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:  phi3.rope.scaling.original_context_length u32              = 4096\n",
      "llama_model_loader: - kv   7:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   8:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                  phi3.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  11:               phi3.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  13:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  14:                        phi3.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  16:              phi3.attention.sliding_window u32              = 262144\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = gpt-4o\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,200064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,200064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,199742]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"e r\", ...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 199999\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 199999\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.unknown_token_id u32              = 199999\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 199999\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - type  f32:   67 tensors\n",
      "llama_model_loader: - type  f16:  129 tensors\n",
      "load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)\n",
      "[   1/ 196]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   2/ 196]             rope_factors_long.weight - [   48,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 196]            rope_factors_short.weight - [   48,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   4/ 196]                    token_embd.weight - [ 3072, 200064,     1,     1], type =    f16, converting to q6_K .. size =  1172.25 MiB ->   480.81 MiB\n",
      "[   5/ 196]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   6/ 196]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[   7/ 196]                blk.0.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[   8/ 196]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[   9/ 196]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  10/ 196]                  blk.0.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  11/ 196]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 196]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  13/ 196]                blk.1.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  14/ 196]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  15/ 196]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  16/ 196]                  blk.1.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  17/ 196]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  18/ 196]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  19/ 196]                blk.2.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  20/ 196]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  21/ 196]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  22/ 196]                  blk.2.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  23/ 196]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  24/ 196]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  25/ 196]                blk.3.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  26/ 196]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  27/ 196]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  28/ 196]                  blk.3.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  29/ 196]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 196]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  31/ 196]                blk.4.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  32/ 196]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  33/ 196]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  34/ 196]                  blk.4.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  35/ 196]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  36/ 196]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  37/ 196]                blk.5.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  38/ 196]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  39/ 196]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  40/ 196]                  blk.5.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  41/ 196]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  42/ 196]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  43/ 196]                blk.6.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  44/ 196]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  45/ 196]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  46/ 196]                  blk.6.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  47/ 196]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 196]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  49/ 196]                blk.7.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  50/ 196]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  51/ 196]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  52/ 196]                  blk.7.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  53/ 196]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  54/ 196]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  55/ 196]                blk.8.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  56/ 196]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  57/ 196]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  58/ 196]                  blk.8.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  59/ 196]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  60/ 196]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  61/ 196]                blk.9.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  62/ 196]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  63/ 196]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  64/ 196]                  blk.9.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  65/ 196]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 196]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  67/ 196]               blk.10.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  68/ 196]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  69/ 196]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  70/ 196]                 blk.10.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  71/ 196]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  72/ 196]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  73/ 196]               blk.11.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  74/ 196]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  75/ 196]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  76/ 196]                 blk.11.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  77/ 196]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  78/ 196]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  79/ 196]               blk.12.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  80/ 196]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  81/ 196]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  82/ 196]                 blk.12.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  83/ 196]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 196]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  85/ 196]               blk.13.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  86/ 196]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  87/ 196]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  88/ 196]                 blk.13.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  89/ 196]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  90/ 196]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  91/ 196]               blk.14.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  92/ 196]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  93/ 196]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  94/ 196]                 blk.14.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[  95/ 196]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  96/ 196]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  97/ 196]               blk.15.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[  98/ 196]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  99/ 196]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 100/ 196]                 blk.15.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 101/ 196]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 196]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 103/ 196]               blk.16.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 104/ 196]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 105/ 196]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 106/ 196]                 blk.16.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 107/ 196]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 108/ 196]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 109/ 196]               blk.17.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 110/ 196]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 111/ 196]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 112/ 196]                 blk.17.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 113/ 196]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 196]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 115/ 196]               blk.18.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 116/ 196]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 117/ 196]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 118/ 196]                 blk.18.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 119/ 196]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 196]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 121/ 196]               blk.19.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 122/ 196]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 123/ 196]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 124/ 196]                 blk.19.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 125/ 196]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 126/ 196]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 127/ 196]               blk.20.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 128/ 196]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 129/ 196]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 130/ 196]                 blk.20.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 131/ 196]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 132/ 196]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 133/ 196]               blk.21.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 134/ 196]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 135/ 196]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 136/ 196]                 blk.21.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 137/ 196]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 138/ 196]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 139/ 196]               blk.22.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 140/ 196]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 141/ 196]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 142/ 196]                 blk.22.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 143/ 196]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 144/ 196]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 145/ 196]               blk.23.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 146/ 196]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 147/ 196]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 148/ 196]                 blk.23.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 149/ 196]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 150/ 196]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 151/ 196]               blk.24.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 152/ 196]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 153/ 196]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 154/ 196]                 blk.24.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 155/ 196]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 196]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 157/ 196]               blk.25.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 158/ 196]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 159/ 196]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 160/ 196]                 blk.25.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 161/ 196]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 162/ 196]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 163/ 196]               blk.26.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 164/ 196]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 165/ 196]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 166/ 196]                 blk.26.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 167/ 196]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 168/ 196]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 169/ 196]               blk.27.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 170/ 196]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 171/ 196]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 172/ 196]                 blk.27.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 173/ 196]              blk.28.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 196]            blk.28.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 175/ 196]               blk.28.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 176/ 196]               blk.28.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 177/ 196]               blk.28.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 178/ 196]                 blk.28.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 179/ 196]              blk.29.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 180/ 196]            blk.29.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 181/ 196]               blk.29.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 182/ 196]               blk.29.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 183/ 196]               blk.29.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 184/ 196]                 blk.29.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 185/ 196]              blk.30.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 186/ 196]            blk.30.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 187/ 196]               blk.30.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 188/ 196]               blk.30.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 189/ 196]               blk.30.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 190/ 196]                 blk.30.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "[ 191/ 196]              blk.31.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 196]            blk.31.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 193/ 196]               blk.31.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q5_K .. size =    30.00 MiB ->    10.31 MiB\n",
      "[ 194/ 196]               blk.31.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 195/ 196]               blk.31.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 196/ 196]                 blk.31.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q4_K .. size =    96.00 MiB ->    27.00 MiB\n",
      "llama_model_quantize_impl: model size  =  7317.01 MB\n",
      "llama_model_quantize_impl: quant size  =  2368.57 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main: quantize time = 40877.65 ms\n",
      "main:    total time = 40877.65 ms\n",
      "[✓] Quantifié Q4_K_M -> gguf_out/phi4_merged/model-Q4_K_M.gguf (2491.9 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main: build = 0 (unknown)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing 'gguf_out/phi4_merged/model-f16.gguf' to 'gguf_out/phi4_merged/model-Q5_K_M.gguf' as Q5_K_M\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 196 tensors from gguf_out/phi4_merged/model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
      "llama_model_loader: - kv   2:                               general.type str              = model\n",
      "llama_model_loader: - kv   3:                               general.name str              = Merged_Hf\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 3.8B\n",
      "llama_model_loader: - kv   5:                        phi3.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:  phi3.rope.scaling.original_context_length u32              = 4096\n",
      "llama_model_loader: - kv   7:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   8:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                  phi3.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  11:               phi3.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  13:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  14:                        phi3.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  16:              phi3.attention.sliding_window u32              = 262144\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = gpt-4o\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,200064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,200064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,199742]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"e r\", ...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 199999\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 199999\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.unknown_token_id u32              = 199999\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 199999\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - type  f32:   67 tensors\n",
      "llama_model_loader: - type  f16:  129 tensors\n",
      "load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)\n",
      "[   1/ 196]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   2/ 196]             rope_factors_long.weight - [   48,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 196]            rope_factors_short.weight - [   48,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   4/ 196]                    token_embd.weight - [ 3072, 200064,     1,     1], type =    f16, converting to q6_K .. size =  1172.25 MiB ->   480.81 MiB\n",
      "[   5/ 196]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   6/ 196]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[   7/ 196]                blk.0.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[   8/ 196]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[   9/ 196]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  10/ 196]                  blk.0.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  11/ 196]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 196]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  13/ 196]                blk.1.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  14/ 196]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  15/ 196]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  16/ 196]                  blk.1.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  17/ 196]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  18/ 196]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  19/ 196]                blk.2.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  20/ 196]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  21/ 196]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  22/ 196]                  blk.2.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  23/ 196]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  24/ 196]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  25/ 196]                blk.3.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  26/ 196]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  27/ 196]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  28/ 196]                  blk.3.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  29/ 196]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 196]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  31/ 196]                blk.4.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  32/ 196]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  33/ 196]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  34/ 196]                  blk.4.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  35/ 196]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  36/ 196]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  37/ 196]                blk.5.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  38/ 196]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  39/ 196]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  40/ 196]                  blk.5.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  41/ 196]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  42/ 196]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  43/ 196]                blk.6.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  44/ 196]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  45/ 196]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  46/ 196]                  blk.6.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  47/ 196]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 196]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  49/ 196]                blk.7.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  50/ 196]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  51/ 196]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  52/ 196]                  blk.7.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  53/ 196]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  54/ 196]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  55/ 196]                blk.8.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  56/ 196]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  57/ 196]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  58/ 196]                  blk.8.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  59/ 196]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  60/ 196]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  61/ 196]                blk.9.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  62/ 196]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  63/ 196]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  64/ 196]                  blk.9.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  65/ 196]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 196]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  67/ 196]               blk.10.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  68/ 196]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  69/ 196]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  70/ 196]                 blk.10.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  71/ 196]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  72/ 196]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  73/ 196]               blk.11.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  74/ 196]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  75/ 196]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  76/ 196]                 blk.11.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  77/ 196]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  78/ 196]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  79/ 196]               blk.12.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  80/ 196]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  81/ 196]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  82/ 196]                 blk.12.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  83/ 196]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 196]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  85/ 196]               blk.13.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  86/ 196]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  87/ 196]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  88/ 196]                 blk.13.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  89/ 196]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  90/ 196]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  91/ 196]               blk.14.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  92/ 196]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  93/ 196]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  94/ 196]                 blk.14.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[  95/ 196]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  96/ 196]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  97/ 196]               blk.15.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  98/ 196]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  99/ 196]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 100/ 196]                 blk.15.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 101/ 196]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 196]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 103/ 196]               blk.16.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 104/ 196]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 105/ 196]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 106/ 196]                 blk.16.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 107/ 196]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 108/ 196]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 109/ 196]               blk.17.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 110/ 196]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 111/ 196]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 112/ 196]                 blk.17.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 113/ 196]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 196]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 115/ 196]               blk.18.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 116/ 196]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 117/ 196]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 118/ 196]                 blk.18.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 119/ 196]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 196]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 121/ 196]               blk.19.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 122/ 196]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 123/ 196]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 124/ 196]                 blk.19.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 125/ 196]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 126/ 196]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 127/ 196]               blk.20.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 128/ 196]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 129/ 196]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 130/ 196]                 blk.20.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 131/ 196]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 132/ 196]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 133/ 196]               blk.21.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 134/ 196]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 135/ 196]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 136/ 196]                 blk.21.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 137/ 196]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 138/ 196]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 139/ 196]               blk.22.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 140/ 196]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 141/ 196]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 142/ 196]                 blk.22.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 143/ 196]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 144/ 196]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 145/ 196]               blk.23.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 146/ 196]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 147/ 196]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 148/ 196]                 blk.23.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 149/ 196]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 150/ 196]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 151/ 196]               blk.24.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 152/ 196]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 153/ 196]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 154/ 196]                 blk.24.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 155/ 196]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 196]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 157/ 196]               blk.25.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 158/ 196]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 159/ 196]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 160/ 196]                 blk.25.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 161/ 196]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 162/ 196]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 163/ 196]               blk.26.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 164/ 196]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 165/ 196]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 166/ 196]                 blk.26.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 167/ 196]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 168/ 196]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 169/ 196]               blk.27.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 170/ 196]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 171/ 196]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 172/ 196]                 blk.27.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 173/ 196]              blk.28.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 196]            blk.28.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 175/ 196]               blk.28.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 176/ 196]               blk.28.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 177/ 196]               blk.28.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 178/ 196]                 blk.28.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 179/ 196]              blk.29.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 180/ 196]            blk.29.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 181/ 196]               blk.29.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 182/ 196]               blk.29.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 183/ 196]               blk.29.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 184/ 196]                 blk.29.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 185/ 196]              blk.30.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 186/ 196]            blk.30.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 187/ 196]               blk.30.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 188/ 196]               blk.30.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 189/ 196]               blk.30.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 190/ 196]                 blk.30.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "[ 191/ 196]              blk.31.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 196]            blk.31.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 193/ 196]               blk.31.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 194/ 196]               blk.31.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 195/ 196]               blk.31.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 196/ 196]                 blk.31.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q5_K .. size =    96.00 MiB ->    33.00 MiB\n",
      "llama_model_quantize_impl: model size  =  7317.01 MB\n",
      "llama_model_quantize_impl: quant size  =  2708.32 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main: quantize time = 30997.43 ms\n",
      "main:    total time = 30997.43 ms\n",
      "[✓] Quantifié Q5_K_M -> gguf_out/phi4_merged/model-Q5_K_M.gguf (2848.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main: build = 0 (unknown)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing 'gguf_out/phi4_merged/model-f16.gguf' to 'gguf_out/phi4_merged/model-Q6_K.gguf' as Q6_K\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 196 tensors from gguf_out/phi4_merged/model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
      "llama_model_loader: - kv   2:                               general.type str              = model\n",
      "llama_model_loader: - kv   3:                               general.name str              = Merged_Hf\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 3.8B\n",
      "llama_model_loader: - kv   5:                        phi3.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:  phi3.rope.scaling.original_context_length u32              = 4096\n",
      "llama_model_loader: - kv   7:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   8:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                  phi3.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  11:               phi3.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  13:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  14:                        phi3.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  16:              phi3.attention.sliding_window u32              = 262144\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = gpt-4o\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,200064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,200064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,199742]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"e r\", ...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 199999\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 199999\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.unknown_token_id u32              = 199999\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 199999\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - type  f32:   67 tensors\n",
      "llama_model_loader: - type  f16:  129 tensors\n",
      "load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)\n",
      "[   1/ 196]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   2/ 196]             rope_factors_long.weight - [   48,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 196]            rope_factors_short.weight - [   48,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   4/ 196]                    token_embd.weight - [ 3072, 200064,     1,     1], type =    f16, converting to q6_K .. size =  1172.25 MiB ->   480.81 MiB\n",
      "[   5/ 196]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   6/ 196]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[   7/ 196]                blk.0.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[   8/ 196]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[   9/ 196]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  10/ 196]                  blk.0.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  11/ 196]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 196]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  13/ 196]                blk.1.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  14/ 196]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  15/ 196]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  16/ 196]                  blk.1.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  17/ 196]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  18/ 196]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  19/ 196]                blk.2.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  20/ 196]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  21/ 196]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  22/ 196]                  blk.2.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  23/ 196]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  24/ 196]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  25/ 196]                blk.3.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  26/ 196]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  27/ 196]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  28/ 196]                  blk.3.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  29/ 196]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 196]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  31/ 196]                blk.4.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  32/ 196]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  33/ 196]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  34/ 196]                  blk.4.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  35/ 196]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  36/ 196]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  37/ 196]                blk.5.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  38/ 196]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  39/ 196]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  40/ 196]                  blk.5.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  41/ 196]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  42/ 196]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  43/ 196]                blk.6.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  44/ 196]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  45/ 196]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  46/ 196]                  blk.6.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  47/ 196]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 196]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  49/ 196]                blk.7.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  50/ 196]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  51/ 196]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  52/ 196]                  blk.7.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  53/ 196]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  54/ 196]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  55/ 196]                blk.8.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  56/ 196]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  57/ 196]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  58/ 196]                  blk.8.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  59/ 196]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  60/ 196]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  61/ 196]                blk.9.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  62/ 196]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  63/ 196]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  64/ 196]                  blk.9.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  65/ 196]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 196]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  67/ 196]               blk.10.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  68/ 196]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  69/ 196]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  70/ 196]                 blk.10.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  71/ 196]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  72/ 196]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  73/ 196]               blk.11.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  74/ 196]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  75/ 196]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  76/ 196]                 blk.11.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  77/ 196]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  78/ 196]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  79/ 196]               blk.12.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  80/ 196]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  81/ 196]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  82/ 196]                 blk.12.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  83/ 196]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 196]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  85/ 196]               blk.13.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  86/ 196]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  87/ 196]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  88/ 196]                 blk.13.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  89/ 196]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  90/ 196]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  91/ 196]               blk.14.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  92/ 196]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  93/ 196]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  94/ 196]                 blk.14.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[  95/ 196]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  96/ 196]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  97/ 196]               blk.15.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[  98/ 196]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  99/ 196]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 100/ 196]                 blk.15.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 101/ 196]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 196]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 103/ 196]               blk.16.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 104/ 196]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 105/ 196]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 106/ 196]                 blk.16.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 107/ 196]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 108/ 196]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 109/ 196]               blk.17.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 110/ 196]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 111/ 196]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 112/ 196]                 blk.17.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 113/ 196]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 196]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 115/ 196]               blk.18.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 116/ 196]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 117/ 196]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 118/ 196]                 blk.18.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 119/ 196]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 196]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 121/ 196]               blk.19.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 122/ 196]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 123/ 196]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 124/ 196]                 blk.19.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 125/ 196]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 126/ 196]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 127/ 196]               blk.20.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 128/ 196]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 129/ 196]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 130/ 196]                 blk.20.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 131/ 196]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 132/ 196]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 133/ 196]               blk.21.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 134/ 196]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 135/ 196]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 136/ 196]                 blk.21.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 137/ 196]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 138/ 196]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 139/ 196]               blk.22.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 140/ 196]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 141/ 196]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 142/ 196]                 blk.22.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 143/ 196]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 144/ 196]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 145/ 196]               blk.23.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 146/ 196]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 147/ 196]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 148/ 196]                 blk.23.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 149/ 196]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 150/ 196]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 151/ 196]               blk.24.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 152/ 196]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 153/ 196]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 154/ 196]                 blk.24.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 155/ 196]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 196]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 157/ 196]               blk.25.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 158/ 196]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 159/ 196]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 160/ 196]                 blk.25.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 161/ 196]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 162/ 196]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 163/ 196]               blk.26.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 164/ 196]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 165/ 196]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 166/ 196]                 blk.26.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 167/ 196]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 168/ 196]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 169/ 196]               blk.27.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 170/ 196]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 171/ 196]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 172/ 196]                 blk.27.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 173/ 196]              blk.28.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 196]            blk.28.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 175/ 196]               blk.28.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 176/ 196]               blk.28.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 177/ 196]               blk.28.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 178/ 196]                 blk.28.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 179/ 196]              blk.29.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 180/ 196]            blk.29.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 181/ 196]               blk.29.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 182/ 196]               blk.29.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 183/ 196]               blk.29.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 184/ 196]                 blk.29.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 185/ 196]              blk.30.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 186/ 196]            blk.30.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 187/ 196]               blk.30.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 188/ 196]               blk.30.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 189/ 196]               blk.30.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 190/ 196]                 blk.30.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "[ 191/ 196]              blk.31.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 196]            blk.31.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 193/ 196]               blk.31.attn_qkv.weight - [ 3072,  5120,     1,     1], type =    f16, converting to q6_K .. size =    30.00 MiB ->    12.30 MiB\n",
      "[ 194/ 196]               blk.31.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 195/ 196]               blk.31.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 196/ 196]                 blk.31.ffn_up.weight - [ 3072, 16384,     1,     1], type =    f16, converting to q6_K .. size =    96.00 MiB ->    39.38 MiB\n",
      "llama_model_quantize_impl: model size  =  7317.01 MB\n",
      "llama_model_quantize_impl: quant size  =  3001.57 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main: quantize time = 20994.98 ms\n",
      "main:    total time = 20994.98 ms\n",
      "[✓] Quantifié Q6_K -> gguf_out/phi4_merged/model-Q6_K.gguf (3155.6 MB)\n",
      "\n",
      "[Résumé sorties]\n",
      " - model-Q4_K_M.gguf     2491.9 MB\n",
      " - model-Q5_K_M.gguf     2848.1 MB\n",
      " - model-Q6_K.gguf       3155.6 MB\n",
      " - model-f16.gguf        7680.7 MB\n",
      "\n",
      "[Conseil] Test rapide :\n",
      "  /workspace/llama.cpp/build/bin/main -m gguf_out/phi4_merged/model-Q4_K_M.gguf -p 'Dites bonjour.' -n 64\n"
     ]
    }
   ],
   "source": [
    "# === Fusion LoRA -> HF, conversion GGUF f16 et quantification llama.cpp ===\n",
    "import os, sys, shutil, subprocess\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- chemins fixes ---\n",
    "MODEL_PATH = \"models/phi4\"\n",
    "ADAPTER_DIR = \"checkpoints_phi4_lora\"\n",
    "LLAMA_CPP = \"/workspace/llama.cpp\"\n",
    "CONVERT_SCRIPT = f\"{LLAMA_CPP}/convert_hf_to_gguf.py\"\n",
    "QUANT_BIN = f\"{LLAMA_CPP}/build/bin/llama-quantize\"\n",
    "QUANT_RUN = f\"{LLAMA_CPP}/build/bin/llama-run\"\n",
    "OUT_DIR = Path(\"gguf_out/phi4_merged\")\n",
    "GGUF_OUTTYPE = \"f16\"\n",
    "QUANTS = [\"Q4_K_M\", \"Q5_K_M\", \"Q6_K\"]\n",
    "\n",
    "# --- chargement et fusion ---\n",
    "print(f\"[i] Chargement du modèle : {MODEL_PATH}\")\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True, local_files_only=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "    tok.pad_token_id = tok.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=torch.float32,\n",
    "    device_map=\"cpu\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "print(f\"[i] Chargement adaptateur LoRA depuis {ADAPTER_DIR}\")\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_DIR, is_trainable=False)\n",
    "print(\"[i] Fusion (merge_and_unload)...\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# --- sauvegarde HF fusionné ---\n",
    "MERGED_DIR = OUT_DIR / \"merged_hf\"\n",
    "MERGED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(MERGED_DIR)\n",
    "tok.save_pretrained(MERGED_DIR)\n",
    "print(f\"[✓] Modèle fusionné sauvegardé : {MERGED_DIR}\")\n",
    "\n",
    "# --- conversion HF → GGUF f16 ---\n",
    "gguf_f16 = OUT_DIR / f\"model-{GGUF_OUTTYPE}.gguf\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "subprocess.run([\n",
    "    sys.executable, CONVERT_SCRIPT,\n",
    "    \"--outtype\", GGUF_OUTTYPE,\n",
    "    \"--outfile\", str(gguf_f16),\n",
    "    str(MERGED_DIR)\n",
    "], check=True)\n",
    "\n",
    "print(f\"[✓] Conversion GGUF f16 : {gguf_f16} ({gguf_f16.stat().st_size/1e6:.1f} MB)\")\n",
    "\n",
    "# --- quantification ---\n",
    "for q in QUANTS:\n",
    "    out_q = OUT_DIR / f\"model-{q}.gguf\"\n",
    "    subprocess.run([QUANT_BIN, str(gguf_f16), str(out_q), q], check=True)\n",
    "    print(f\"[✓] Quantifié {q} -> {out_q} ({out_q.stat().st_size/1e6:.1f} MB)\")\n",
    "\n",
    "print(\"\\n[Résumé sorties]\")\n",
    "for f in sorted(OUT_DIR.glob(\"model-*.gguf\")):\n",
    "    print(f\" - {f.name:20s}  {f.stat().st_size/1e6:.1f} MB\")\n",
    "\n",
    "print(\"\\n[Conseil] Test rapide :\")\n",
    "print(f\"  {LLAMA_CPP}/build/bin/main -n 64 {OUT_DIR}/model-Q4_K_M.gguf 'Dites bonjour.' -n 64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67991390-20ce-40b4-804e-c66698744b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salut!\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "LLAMA_RUN = \"/workspace/llama.cpp/build/bin/llama-run\"\n",
    "OUT_DIR = Path(\"/workspace/lora_local/gguf_out/phi4_merged\")\n",
    "MODEL = f\"{OUT_DIR}/model-Q4_K_M.gguf\"\n",
    "\n",
    "result = subprocess.run(\n",
    "    [LLAMA_RUN, \"-n\", \"64\", MODEL, \"Dites bonjour en un seul mot.\"],\n",
    "    text=True,\n",
    "    capture_output=True\n",
    ")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d526c5-036b-4379-99ba-29c2fb3ec17b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
